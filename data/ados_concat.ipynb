{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "from itertools import combinations\n",
    "\n",
    "def get_filenames(path, exclude=[]):\n",
    "    \"\"\"\n",
    "    Returns list of datasets filenames in path\n",
    "    \"\"\"\n",
    "    directory = \"ados_datasets/\"\n",
    "    exclude = []\n",
    "    files = [join(directory, f) for f in listdir(directory) \\\n",
    "             if isfile(join(directory, f)) \\\n",
    "             and f.startswith(\"ados\") and f.endswith(\".txt\") and f not in exclude]\n",
    "    return files\n",
    "\n",
    "def delete_nan_columns(df, max_nan_ratio):\n",
    "    \"\"\"\n",
    "    Delete columns from df with more than max_nan_ratio of NaN values\n",
    "    \"\"\"\n",
    "    for col in sorted(df.columns):\n",
    "        count_nan = df[col].isnull().sum()\n",
    "        ratio_nan = count_nan / len(df)\n",
    "        if ratio_nan > max_nan_ratio:\n",
    "            del df[col]\n",
    "            \n",
    "def has_required_columns(df, required_columns):\n",
    "    \"\"\"\n",
    "    Check if df has required_columns\n",
    "    \"\"\"\n",
    "    return set(required_columns) < set(df.columns)\n",
    "\n",
    "def remove_preffix(df, col):\n",
    "    \"\"\"\n",
    "    Handles appearances of preffixes in columns names\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"^[a-eA-E]{1}[0-9]{1,2}[aA]?\\.\")\n",
    "    match = pattern.match(col)\n",
    "    if match:\n",
    "        clean_col = col[match.span()[1]:].strip()\n",
    "        if clean_col.lower() in map(str.lower, df.columns):\n",
    "            del df[col]\n",
    "        else:\n",
    "            df.rename(columns = {col : clean_col}, inplace=True)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def remove_suffix(df, col):\n",
    "    \"\"\"\n",
    "    Handles appearances of suffixes in columns names\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\".*\\.[1-9]$\")\n",
    "    match = pattern.match(col)\n",
    "    if match:\n",
    "        if col[:-2] in df:\n",
    "            del df[col]\n",
    "        else:\n",
    "            df.rename(columns = {col : col[:-2]}, inplace=True)\n",
    "            \n",
    "def similarity(a, b):\n",
    "    \"\"\"\n",
    "    Returns similarity ratio between strings a and b\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def generate_similar_names(df, min_similarity=0.9):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of similar names within df.columns\n",
    "    \"\"\"\n",
    "    return {c1: c2 for (c1, c2) in combinations(df.columns, 2) if similarity(c1, c2) > min_similarity}\n",
    "            \n",
    "def merge_similar_columns(df, similar_names):\n",
    "    \"\"\"\n",
    "    Merges columns with similar names, using the non-NaN values if possible\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col in similar_names:\n",
    "            # Combine both columns into the first one\n",
    "            df[col] = df[col].fillna(df[similar_names[col]])\n",
    "            # Delete the second column\n",
    "            del df[similar_names[col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding file: ados_datasets/ados1_200102_1250.txt\n",
      "Adding file: ados_datasets/ados3_200102_2382.txt\n",
      "Adding file: ados_datasets/ados1_201201_2368.txt\n",
      "Adding file: ados_datasets/ados4_200102_2382.txt\n",
      "Adding file: ados_datasets/ados2_201201_1250.txt\n",
      "Adding file: ados_datasets/ados2_200102_9.txt\n",
      "Adding file: ados_datasets/ados2_200102.txt\n",
      "Adding file: ados_datasets/ados2_200701_1946.txt\n",
      "Adding file: ados_datasets/ados1_200701_1250.txt\n",
      "Adding file: ados_datasets/ados2_200102_2382.txt\n",
      "Adding file: ados_datasets/ados1_201201_2080.txt\n",
      "Adding file: ados_datasets/ados1_201201_1946.txt\n",
      "Adding file: ados_datasets/ados2_201201_2368.txt\n",
      "Adding file: ados_datasets/ados1_200102_9.txt\n",
      "Adding file: ados_datasets/ados2_201201_19.txt\n",
      "Adding file: ados_datasets/ados1_200102.txt\n",
      "Adding file: ados_datasets/ados2_200102_1250.txt\n",
      "Adding file: ados_datasets/ados1_201201_19.txt\n",
      "Adding file: ados_datasets/ados3_201201_1946.txt\n",
      "Adding file: ados_datasets/ados3_201201_2080.txt\n",
      "Adding file: ados_datasets/ados3_200102_1250.txt\n",
      "Adding file: ados_datasets/ados1_200102_2382.txt\n",
      "Adding file: ados_datasets/ados2_200701_1250.txt\n",
      "Adding file: ados_datasets/ados2_201201_1946.txt\n",
      "Adding file: ados_datasets/ados2_201201_2080.txt\n",
      "Adding file: ados_datasets/ados3_201201_2368.txt\n",
      "Adding file: ados_datasets/ados1_201201_1250.txt\n",
      "Adding file: ados_datasets/ados1_200701_1946.txt\n",
      "Adding file: ados_datasets/ados1_200102_19.txt\n",
      "\n",
      "SIMILAR NAMES: {\n",
      " \"eye contact\": \"eye contact:\",\n",
      " \"overall level of non-echoed language\": \"overall level of non-echoed spoken language\",\n",
      " \"pointing with index finger\": \"points with index finger\",\n",
      " \"specify\": \"specify:\",\n",
      " \"unusual sensory interest in material/person\": \"unusual sensory interest in play material/person\"\n",
      "}\n",
      "\n",
      "COLUMNS:\n",
      "ados diagnosis classification\n",
      "age in months at the time of the interview/test/sampling/imaging.\n",
      "anxiety\n",
      "collection_id\n",
      "collection_title\n",
      "dataset_id\n",
      "date on which the interview/genetic test/sampling/imaging/biospecimen was completed. mm/dd/yyyy\n",
      "hand and finger and other complex mannerisms\n",
      "imagination/creativity\n",
      "immediate echolalia\n",
      "overall ados diagnosis\n",
      "overall level of non-echoed language\n",
      "promoted_subjectkey\n",
      "quality of social overtures\n",
      "self-injurious behavior\n",
      "sex of the subject\n",
      "shared enjoyment in interaction\n",
      "subject id how it's defined in lab/project\n",
      "tantrums, aggression, negative or disruptive behavior\n",
      "the ndar global unique identifier (guid) for research subject\n",
      "unusual eye contact\n",
      "\n",
      "TOTAL COLUMNS: 21\n",
      "\n",
      "Number of rows with...\n",
      "\t... 0 NaN features: 2733\n",
      "\t... 1 NaN features: 331\n",
      "\t... 2 NaN features: 3\n",
      "\t... 3 NaN features: 19\n",
      "\t... 4 NaN features: 9\n",
      "\t... 5 NaN features: 0\n",
      "\t... 6 NaN features: 0\n",
      "\t... 7 NaN features: 0\n",
      "\t... 8 NaN features: 0\n",
      "\t... 9 NaN features: 698\n",
      "\t... 10 NaN features: 215\n",
      "\t... 11 NaN features: 0\n",
      "\t... 12 NaN features: 0\n",
      "\t... 13 NaN features: 64\n",
      "\t... 14 NaN features: 0\n",
      "\t... 15 NaN features: 0\n",
      "\t... 16 NaN features: 0\n",
      "\t... 17 NaN features: 0\n",
      "\t... 18 NaN features: 0\n",
      "\t... 19 NaN features: 0\n",
      "\t... 20 NaN features: 0\n",
      "\t... 21 NaN features: 0\n",
      "\n",
      "TOTAL ROWS: 4072\n"
     ]
    }
   ],
   "source": [
    "files = get_filenames(\"ados_datasets/\")\n",
    "    \n",
    "required_columns = [\n",
    "    \"ADOS Diagnosis Classification\"\n",
    "]\n",
    "\n",
    "concat_df = pd.DataFrame()  # Concatenation of clean dataframes\n",
    "for f in files:\n",
    "    df = pd.read_table(f, header=1, sep=\"\\t\")  # Read dataset using 2nd row values as columns headers\n",
    "    \n",
    "    \"\"\"\n",
    "    if not has_required_columns(df, required_columns):\n",
    "        continue\n",
    "    \"\"\"\n",
    "    # Delete columns with more than 80% of NaN values\n",
    "    delete_nan_columns(concat_df, max_nan_ratio=0.80)\n",
    "    \n",
    "    # Remove duplicate columns (just keep one of them)\n",
    "    lower_columns = [x.lower() for x in df.columns]\n",
    "    for col in sorted(df.columns):\n",
    "        # Check for columns ending with \".1\" or \".2\" or ...\n",
    "        remove_suffix(df, col)\n",
    "                \n",
    "        # Remove duplicated columns\n",
    "        if lower_columns.count(col.lower()) > 1:\n",
    "            del df[col]\n",
    "            \n",
    "        # Check for columns starting with a \"A12.a\" pattern and rename them\n",
    "        remove_preffix(df, col)\n",
    "        \n",
    "    # Transform columns names to lowercase\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    \n",
    "    # Concatenate dataframes\n",
    "    if concat_df.empty:\n",
    "        concat_df = df\n",
    "    else:\n",
    "        concat_df = pd.concat([concat_df, df], join=\"outer\", ignore_index=True)\n",
    "    \n",
    "    print(\"Adding file: {}\".format(f))\n",
    "\n",
    "# Generate similar names dictionary\n",
    "similar_names = generate_similar_names(concat_df)\n",
    "print(\"\\nSIMILAR NAMES:\", json.dumps(similar_names, indent=1))\n",
    "\n",
    "# Add special cases (not similar names, but corresponding columns)\n",
    "# TODO\n",
    "    \n",
    "# Merge columns with similar names\n",
    "merge_similar_columns(concat_df, similar_names)\n",
    "\n",
    "# Delete columns with more than 25% of NaN values\n",
    "delete_nan_columns(concat_df, 0.25)\n",
    "    \n",
    "# Stats\n",
    "print(\"\\nCOLUMNS:\")\n",
    "for col in sorted(concat_df.columns):\n",
    "    print(col)\n",
    "print(\"\\nTOTAL COLUMNS:\", len(concat_df.columns))\n",
    "    \n",
    "concat_df\n",
    "print(\"\\nNumber of rows with...\")\n",
    "for i in range(len(concat_df.columns) + 1):\n",
    "    non_nan_rows = np.sum(concat_df.isnull().sum(axis=1) == i)\n",
    "    print(\"\\t... {} NaN features: {}\".format(i, non_nan_rows))\n",
    "print(\"\\nTOTAL ROWS:\", len(concat_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
